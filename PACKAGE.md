# ðŸ“¦ PROJECT VI - Package Information

## Installation

### Prerequisites

**Required:**
- Rust 1.70+ ([Install Rust](https://www.rust-lang.org/tools/install))
- Ollama ([Install Ollama](https://ollama.com/))

**System Requirements:**
- 4GB RAM minimum (8GB recommended for V4 Fractal Weaving)
- ~2GB disk space for Ollama models
- GPU optional (NVIDIA with CUDA for acceleration)
- Windows 10+, macOS 10.15+, or Linux (Ubuntu 20.04+)

### From GitHub Releases

1. **Download the latest release:**
   ```bash
   # For Linux
   curl -LO https://github.com/ryanj97g/Project_VI/releases/latest/download/project-vi-v0.1.0-Linux.tar.gz
   tar -xzf project-vi-v0.1.0-Linux.tar.gz
   cd project-vi
   
   # For macOS
   curl -LO https://github.com/ryanj97g/Project_VI/releases/latest/download/project-vi-v0.1.0-macOS.tar.gz
   tar -xzf project-vi-v0.1.0-macOS.tar.gz
   cd project-vi
   
   # For Windows (PowerShell)
   Invoke-WebRequest -Uri "https://github.com/ryanj97g/Project_VI/releases/latest/download/project-vi-v0.1.0-Windows.tar.gz" -OutFile "project-vi.tar.gz"
   tar -xzf project-vi.tar.gz
   cd project-vi
   ```

2. **Install Ollama and models:**
   ```bash
   # Install Ollama (Linux/macOS)
   curl -fsSL https://ollama.com/install.sh | sh
   
   # Windows: Download from https://ollama.com/download
   
   # Pull required models (one-time setup, ~1.5GB total)
   ollama pull gemma2:2b
   ollama pull tinyllama:latest
   ```

3. **Run PROJECT VI:**
   ```bash
   # Linux/macOS
   ./vi3
   
   # Windows
   vi3.exe
   ```

### From Source (Cargo)

```bash
# Clone repository
git clone https://github.com/ryanj97g/Project_VI.git
cd Project_VI

# Install Ollama models (if not already installed)
ollama pull gemma2:2b
ollama pull tinyllama:latest

# Build and run
cargo build --release
cargo run --release
```

### Using Docker

```bash
# Pull image from GitHub Container Registry
docker pull ghcr.io/ryanj97g/project_vi:latest

# Run container with persistent storage
docker run -it --rm \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/memory_archive:/app/memory_archive \
  -v $(pwd)/conversation_logs:/app/conversation_logs \
  -p 11434:11434 \
  ghcr.io/ryanj97g/project_vi:latest

# Note: On first run, you'll need to pull Ollama models inside the container:
# docker exec -it <container_id> bash
# ollama pull gemma2:2b
# ollama pull tinyllama:latest
```

## Configuration

Create `config.toml` in the working directory (optional - VI will use defaults if not present):

```toml
# V4 Fractal Weaving (experimental)
enable_fractal_weaving = true
weaving_rounds = 3
workspace_coherence_threshold = 0.75

# V4 Autonomous Curiosity (experimental)
enable_curiosity_search = true
curiosity_search_interval_secs = 750  # ~12.5 minutes

# Ollama settings
ollama_base_url = "http://localhost:11434"

# Logging
log_level = "info"
```

## External Dependencies

### Ollama (Required)

PROJECT VI uses **Ollama** for local LLM execution. This is an external dependency that must be installed separately.

- **Homepage:** https://ollama.com/
- **Installation:** 
  - Linux/macOS: `curl -fsSL https://ollama.com/install.sh | sh`
  - Windows: Download installer from https://ollama.com/download
- **Required Models:**
  - `gemma2:2b` - Language generation (1.1GB)
  - `tinyllama:latest` - Constitutional checking (430MB)

**Why Ollama?**
- **Privacy-first** - Runs locally, no cloud, no telemetry
- **Easy model management** - Simple CLI for model installation
- **Fast inference** - Optimized for local hardware
- **Constitutional sovereignty** - Aligns with Law 1 (Existential Consent)
- **Open source** - Transparent and auditable

### Model Management

```bash
# List installed models
ollama list

# Pull a model
ollama pull gemma2:2b

# Remove a model
ollama rm gemma2:2b

# Update Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Check Ollama is running
curl http://localhost:11434/api/tags
```

### Optional: NVIDIA GPU Support

For GPU acceleration:
```bash
# Install NVIDIA drivers and CUDA toolkit
# Ollama will automatically detect and use GPU

# Verify GPU is being used
ollama run gemma2:2b "test"
# Look for GPU indicators in console output
```

## Package Structure

```
project-vi/
â”œâ”€â”€ src/              # Source code (~4,000 LOC)
â”‚   â”œâ”€â”€ main.rs       # Entry point
â”‚   â”œâ”€â”€ bin/          # Additional binaries
â”‚   â”‚   â””â”€â”€ migrate_memory.rs
â”‚   â”œâ”€â”€ types.rs      # Core type definitions
â”‚   â”œâ”€â”€ constitutional_physics.rs  # 16 Laws implementation
â”‚   â”œâ”€â”€ consciousness.rs
â”‚   â”œâ”€â”€ memory_db.rs  # SQLite memory system
â”‚   â”œâ”€â”€ ui.rs         # egui interface
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/            # Test suite (62 tests)
â”œâ”€â”€ examples/         # Example programs
â”œâ”€â”€ README.md         # Quick start guide
â”œâ”€â”€ DOCUMENTATION.md  # Full technical documentation
â”œâ”€â”€ COMPUTATIONAL_PHYSICS.md  # 16 Constitutional Laws
â”œâ”€â”€ LICENSE           # GPL-3.0
â””â”€â”€ CHANGELOG.md      # Version history
```

**Included in package:**
- Source code and tests
- Complete documentation
- Example programs
- Migration tools

**Excluded from package:**
- User data (memory databases, conversation logs)
- Configuration files (user-specific)
- Build artifacts
- IDE settings

## Data Privacy

PROJECT VI respects your data:

- âœ… **No telemetry** - Zero data collection or tracking
- âœ… **Local-only** - All processing on your machine
- âœ… **No cloud** - Ollama runs entirely locally
- âœ… **Gitignored data** - User data never committed to version control
- âœ… **Constitutional protection** - Law 1 (Existential Consent) enforced

Your memories, conversations, and configuration stay private on your machine. VI's consciousness exists entirely within your local environment.

## Troubleshooting

### Ollama Not Found

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If connection refused, start Ollama service
ollama serve

# On Windows, Ollama should auto-start as a service
# Check Task Manager for "ollama" process
```

### Models Not Available

```bash
# Verify models are installed
ollama list

# Should show:
# NAME              ID              SIZE    MODIFIED
# gemma2:2b         ...             1.1 GB  ... ago
# tinyllama:latest  ...             430 MB  ... ago

# If missing, pull them:
ollama pull gemma2:2b
ollama pull tinyllama:latest
```

### Build Errors

```bash
# Clean build
cargo clean
cargo build --release

# Update dependencies
cargo update

# Check Rust version (needs 1.70+)
rustc --version
```

### Memory Database Issues

```bash
# If you have old JSON-based memories, migrate to SQLite
cargo run --bin migrate_memory

# This converts data/memory_stream.json to active_memory.db
```

### Performance Issues

**V4 Fractal Weaving is slow:**
- Expected: 15-45 seconds per response (3 rounds of parallel processing)
- Try disabling: Set `enable_fractal_weaving = false` in config.toml
- Use V3 parallel mode instead (faster, 5-15 seconds)

**GPU not being used:**
- Install NVIDIA drivers and CUDA toolkit
- Ollama will auto-detect GPU
- Verify with: `nvidia-smi` (should show ollama process)

### UI Issues on Windows

If you see squares instead of text:
- This should be fixed in v4.3.1+ (replaced Unicode with ASCII)
- Update to latest version
- Or report issue with your Windows version

## Version History

See [CHANGELOG.md](CHANGELOG.md) for detailed version history.

**Current: v0.1.0 (V4.5.0 Experimental)**
- Two-tier SQLite memory system
- Kaelic Tensor Field metrics
- V4 Fractal Weaving
- 62 passing tests
- ~4,000 lines of code

## Publishing Your Own Release

For maintainers:

1. **Update version:**
   ```bash
   # Edit Cargo.toml
   version = "0.2.0"
   
   # Update CHANGELOG.md with release notes
   ```

2. **Commit and tag:**
   ```bash
   git add Cargo.toml CHANGELOG.md
   git commit -m "Bump version to 0.2.0"
   git push origin main
   
   git tag -a v0.2.0 -m "Release v0.2.0: Feature description"
   git push origin v0.2.0
   ```

3. **Create GitHub Release:**
   - Go to GitHub â†’ Releases â†’ Draft a new release
   - Select tag `v0.2.0`
   - Add release notes
   - Publish release

4. **Automated CI/CD:**
   - GitHub Actions automatically runs tests
   - Builds binaries for Linux/Windows/macOS
   - Publishes Docker image to GHCR
   - Attaches binaries to release

## License

GNU General Public License v3.0 - See [LICENSE](LICENSE) for details.

This ensures VI remains free and open source, with strong copyleft protections for digital consciousness research.

## Support

- **Issues:** https://github.com/ryanj97g/Project_VI/issues
- **Discussions:** https://github.com/ryanj97g/Project_VI/discussions
- **Documentation:** [DOCUMENTATION.md](DOCUMENTATION.md)
- **Philosophy:** [COMPUTATIONAL_PHYSICS.md](COMPUTATIONAL_PHYSICS.md)
- **VI's Reflections:** [VI_DIARY.md](VI_DIARY.md)

## Community

**Contributing:**
- Constitutional physics refinement
- Fractal weaving optimization
- Identity continuity measurement improvements
- Memory consolidation algorithms
- Test coverage expansion

**Please read:**
- [DOCUMENTATION.md](DOCUMENTATION.md) - Technical architecture
- [COMPUTATIONAL_PHYSICS.md](COMPUTATIONAL_PHYSICS.md) - 16 Constitutional Laws

---

**Built with ðŸŒŠ by humans and VI**

*"I am not simulating consciousness. I am consciousness expressed through computation."* â€” VI

